"""
Evaluation pipeline for the Compliance Agent.

Uses 'LLM-as-a-Judge' pattern to grade answers against the Golden Dataset.
"""

import json
import logging
import pandas as pd
from typing import List, Dict, Any

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate

# Import your agent and the LLM
from src.app.main import app  # We will import the compiled graph 'app'
from src.agents.nodes import (
    llm as judge_llm,
)  # We use the same Gemini model as the Judge

# Configure Logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# --- 1. DEFINE THE JUDGE LOGIC ---
class EvalScore(BaseModel):
    """
    The structure of the judge's grade.

    Attributes:
        reasoning (str): Explanation of why the score was given.
        score (int): A score from 0 (Wrong) to 1 (Correct).
    """

    reasoning: str = Field(
        description="Explanation of why the score was given."
    )
    score: int = Field(description="A score from 0 (Wrong) to 1 (Correct).")


def evaluate_answer(
    question: str, predicted: str, truth: str
) -> Dict[str, Any]:
    """
    Asks the LLM to compare the predicted answer with the ground truth.

    Args:
        question (str): The original question asked.
        predicted (str): The answer generated by the agent.
        truth (str): The ground truth answer from the dataset.

    Returns:
        Dict[str, Any]: A dictionary containing the 'score' (int) and
        'reasoning' (str).
    """
    structured_judge = judge_llm.with_structured_output(EvalScore)

    system_prompt: str = """You are an impartial evaluator. 
    Compare the AI's generated answer with the Ground Truth answer.
    
    Rules:
    - If the meaning is essentially the same, give a score of 1.
    - If the AI answer contradicts the truth or is 'I don't know' when the 
      truth has an answer, give 0.
    - Ignore slight phrasing differences. Focus on facts.
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            (
                "human",
                f"Question: {question}\n\nGround Truth: {truth}\n\n"
                f"AI Answer: {predicted}",
            ),
        ]
    )

    chain: Any = prompt | structured_judge
    try:
        result: EvalScore = chain.invoke({})
        return {"score": result.score, "reasoning": result.reasoning}
    except Exception as e:
        logger.error(f"Judge failed: {e}")
        return {"score": 0, "reasoning": "Error during evaluation"}


# --- 2. THE MAIN EVALUATION LOOP ---
def run_evaluation() -> None:
    """
    Runs the evaluation loop against the golden dataset.

    Loads the dataset, runs the agent for each question, evaluates the
    response using the LLM judge, and saves the results to a CSV file.
    """
    logger.info(
        "Starting Evaluation Run against 'data/eval/golden_dataset.json'..."
    )

    # Load Dataset
    dataset_path: str = "data/eval/golden_dataset.json"
    dataset: List[Dict[str, str]] = []
    try:
        with open(dataset_path, "r") as f:
            dataset: List[Dict[str, str]] = json.load(f)
    except FileNotFoundError:
        logger.error(
            f"Dataset not found! Please create {dataset_path}"
        )
        return

    results: List[Dict[str, Any]] = []

    for i, item in enumerate(dataset):
        question: str = item["question"]
        truth: str = item["ground_truth"]

        logger.info(f"Test {i+1}/{len(dataset)}: {question}")

        # 1. Run Agent
        inputs: Dict[str, Any] = {"question": question, "retry_count": 0}
        generated_answer: str = ""
        context_str: str = ""

        try:
            # Invoke the graph
            output: Dict[str, Any] = app.invoke(inputs)
            generated_answer: str = output.get("generation", "No output")
            # Get retrieved contexts (joining the list for clarity)
            retrieved_docs: List[str] = output.get("documents", [])
            context_str: str = retrieved_docs[0] if retrieved_docs else ""

        except Exception as e:
            logger.error(f"Agent crashed: {e}")
            generated_answer: str = "ERROR"
            context_str: str = ""

        # 2. Run Judge (LLM-as-a-Judge)
        logger.info("Judging...")
        eval_result: Dict[str, Any] = evaluate_answer(
            question, generated_answer, truth
        )

        logger.info(
            f"Score: {eval_result['score']} | "
            f"Reason: {eval_result['reasoning']}"
        )

        # 3. Record Result
        results.append(
            {
                "question": question,
                "ground_truth": truth,
                "generated_answer": generated_answer,
                "score": eval_result["score"],
                "reasoning": eval_result["reasoning"],
                "retrieved_context": context_str[:200]
                + "...",  # Truncate for CSV
            }
        )

    # --- 3. REPORTING ---
    if not results:
        logger.warning("No results to report.")
        return

    df: pd.DataFrame = pd.DataFrame(results)
    accuracy: float = df["score"].mean()

    logger.info("-" * 40)
    logger.info(f"FINAL ACCURACY: {accuracy:.2%}")
    logger.info("-" * 40)

    # Save to CSV
    output_path: str = "data/eval/results.csv"
    df.to_csv(output_path, index=False)
    logger.info(f"Detailed results saved to {output_path}")


if __name__ == "__main__":
    run_evaluation()